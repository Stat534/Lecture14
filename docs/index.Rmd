---
title: "Lecture 14: Spatial GLMs"
output:
  revealjs::revealjs_presentation:
    theme: white
    center: true
    transition: none
    incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(dplyr)
library(ggplot2)
library(knitr)
library(leaflet)
library(gtools)
library(rjags)
library(spBayes)
library(MBA)
library(mgcv)
library(mnormt)
```

# Class Intro

## Intro Questions 
- Describe geometric anisotropy. Discuss this in the context of a data set.

- For Today:
    - Finish geometric anisotropy
    - Spatial Generalized Linear Models

# Geometric Anisotropy

## Geometric Anisotropy Model
- Let $\boldsymbol{Y}(\boldsymbol{s}) = \mu(\boldsymbol{s}) + w(\boldsymbol{s}) + \epsilon(\boldsymbol{s})$,
thus $\boldsymbol{Y}(\boldsymbol{s}) \sim N(\mu(\boldsymbol{s}), \Sigma(\tau^2, \sigma^2, \phi, B))$, where $B = L^T L$.
- The covariance matrix is defined as $\Sigma(\tau^2, \sigma^2, \phi, B)) = \tau^2 I + \sigma^2 H((\boldsymbol{h}^T B \boldsymbol{h}^T)^{\frac{1}{2}}),$ where $H((\boldsymbol{h}^T B \boldsymbol{h}^T)^{\frac{1}{2}})$ has entries of $\rho((\boldsymbol{h_{ij}}^T B \boldsymbol{h_{ij}}^T)^{\frac{1}{2}}))$ with $\rho()$ being a valid correation function, typically including $\phi$ and $\boldsymbol{h_{ij}} = \boldsymbol{s_i} - \boldsymbol{s_j}$.

## Geometric Anisotropy Visual
- Consider four points positioned on a unit circle.
```{r, fig.width=4, fig.height = 4, fig.align = 'center'}
x = c(-1, 0, 0, 1)
y = c(0, -1, 1, 0)
gg_circle <- function(r, xc, yc, color="black", fill=NA, ...) {
    x <- xc + r*cos(seq(0, pi, length.out=100))
    ymax <- yc + r*sin(seq(0, pi, length.out=100))
    ymin <- yc + r*sin(seq(0, -pi, length.out=100))
    annotate("ribbon", x=x, ymin=ymin, ymax=ymax, color=color, fill=fill, ...)
}

data.frame(x=x, y=y) %>% ggplot(aes(x=x,y=y))  + gg_circle(r=1, xc=0, yc=0, color = 'gray') + geom_point(shape = c('1','2','3','4'), size=5)

```

- How far apart are each set of points?

## Geometric Anisotropy Exercise 1
Now consider a set of correlation functions. For each, calculate the correlation matrix and discuss the impact of $B$ on the correlation. Furthermore, how does B change the geometry of the correlation?

1. $\rho() = \exp(-\boldsymbol{h_{ij}}^T B \boldsymbol{h_{ij}}^T)^{\frac{1}{2}})),$ where $B = \begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix}$

2. $\rho() = \exp(-\boldsymbol{h_{ij}}^T B \boldsymbol{h_{ij}}^T)^{\frac{1}{2}})),$ where $B = \begin{pmatrix}
2 & 0 \\
0 & 1 \\
\end{pmatrix}$

3. $\rho() = \exp(-\boldsymbol{h_{ij}}^T B \boldsymbol{h_{ij}}^T)^{\frac{1}{2}})),$ where $B = \begin{pmatrix}
3 & 1 \\
1 & 1 \\
\end{pmatrix}$

## Geometric Anisotropy: Solution 1
```{r}
h.x <- matrix(0, 4, 4)
h.y <- matrix(0, 4, 4)
for (i in 1:4){
  for (j in 1:4){
    h.x[i,j] <- x[i] - x[j]
    h.y[i,j] <- y[i] - y[j]
  }
}
```

$\rho() = \exp(-\boldsymbol{h_{ij}}^T I \boldsymbol{h_{ij}}^T)^{\frac{1}{2}}))$

Implied Distance
```{r}
cor.mat <- matrix(0, 4, 4)
dist.mat <- matrix(0, 4, 4)
for (i in 1:4){
  for (j in 1:4){
    dist.mat[i,j] <- sqrt(t(c(h.x[i,j], h.y[i,j])) %*% diag(2)  %*% (c(h.x[i,j], h.y[i,j])))
    cor.mat[i,j] <- exp(- sqrt(t(c(h.x[i,j], h.y[i,j])) %*% diag(2)  %*% (c(h.x[i,j], h.y[i,j]))) )
  }
}

dist.mat %>% kable(digits = 2)
```

Correlation

```{r}
cor.mat %>% kable(digits = 3)
```

## Geometric Anisotropy: Solution 2

$\rho() = \exp(-\boldsymbol{h_{ij}}^T B \boldsymbol{h_{ij}}^T)^{\frac{1}{2}})),$ where $B = \begin{pmatrix}
2 & 0 \\
0 & 1 \\
\end{pmatrix}$

Implied Distance
```{r}
cor.mat <- matrix(0, 4, 4)
dist.mat <- matrix(0, 4, 4)

for (i in 1:4){
  for (j in 1:4){
    dist.mat[i,j] <- sqrt(t(c(h.x[i,j], h.y[i,j])) %*% matrix(c(2,0,0,1),2,2)  %*% (c(h.x[i,j], h.y[i,j]))) 
    cor.mat[i,j] <- exp(- sqrt(t(c(h.x[i,j], h.y[i,j])) %*% matrix(c(2,0,0,1),2,2)  %*% (c(h.x[i,j], h.y[i,j]))) )
  }
}
dist.mat %>% kable(digits = 2)
```

Correlation
```{r}
cor.mat %>% kable(digits = 3)
```

## Geometric Anisotrop: Solution 3

$\rho() = \exp(-\boldsymbol{h_{ij}}^T B \boldsymbol{h_{ij}}^T)^{\frac{1}{2}})),$ where $B = \begin{pmatrix}
3 & 1 \\
1 & 1 \\
\end{pmatrix}$

Implied Distance
```{r}
cor.mat <- matrix(0, 4, 4)
dist.mat <- matrix(0, 4, 4)
for (i in 1:4){
  for (j in 1:4){
    dist.mat[i,j] <- sqrt(t(c(h.x[i,j], h.y[i,j])) %*% matrix(c(3,1,1,1),2,2)  %*% (c(h.x[i,j], h.y[i,j])))
    cor.mat[i,j] <- exp(- sqrt(t(c(h.x[i,j], h.y[i,j])) %*% matrix(c(3,1,1,1),2,2)  %*% (c(h.x[i,j], h.y[i,j]))) )
  }
}
dist.mat %>% kable(digits = 2)
```

Correlation

```{r}
cor.mat %>% kable(digits = 3)
```


## More Geometric Anisotropy
- The matrix $B$ relates to the orientation of a transformed ellipse.
- The (effective) range for any angle $\eta$ is determined by the equation
$$\rho(r_\eta(\tilde{\boldsymbol{h}}_{\eta}^T B \tilde{\boldsymbol{h}}_{\eta}^T)^{\frac{1}{2}}) = .05,$$
where $\tilde{\boldsymbol{h}}_{\eta}$ is a unit vector in the direction $\eta$.

## Fitting Geometric Anisotropy Models
- Okay, so if we suspect that geometric anisotrophy is present, how do we fit the model? That is, what is necessary in estimating this model?
- In addition to $\sigma^2$ and $\tau^2$ we need to fit $B$.
- What about $\phi$? What is $\phi$ when $B = \begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix}$?

## Priors for B
- While $B$ is a matrix, it is just another unknown parameter.
- Hence, to fit a Bayesian model we need a prior distribution for $B$.
- One option for the positive definite matrix is the Wishart distribution, which is a bit like a matrix-variate gamma distribution.
    
# Spatial GLMS

## Data Viz Motivation

### Ozone Exceedance in Colorado
```{r}
load('CO_Air.Rdata')
pal <- colorFactor(c("green", "red"), domain = c(FALSE, TRUE))
leaflet(CO) %>% addTiles() %>% 
  addCircleMarkers(lng = ~Longitude, lat = ~Latitude,
    color = ~pal(Exceedance))
```

## Generalized Linear Model Notation
There are three components to a generalized linear model:

1. Sampling Distribution: such as Poisson or Binomial
2. Linear combination of predictors: $\eta = X\beta$
3. A link function to map the linear combination of predictors to the support of the sampling distribution.


## Logistic Regression Overview
Write out the complete model specification for logistic regression.

- Assume $Y_i$ is the binary response for the $i^{th}$ observation,
\begin{eqnarray*}
Y_i &\sim& Bernoulli(\pi_i)\\
logit(\pi_i) &=& X_i \beta,
\end{eqnarray*}

- where $logit(\pi_i) = log \left(\frac{\pi_i}{1-\pi_i}\right)$



## `glm()`

Interpret this output

```{r, echo = T}
CO <- CO %>% mutate(north = as.numeric(Latitude > 38 ))
glm(Exceedance~north, family=binomial(link = 'logit'),data=CO) %>% summary
```

## Logistic Regression in JAGS
JAGS specification for logistic regression
```
model{
  # likelihood
	for (i in 1:N){
		y[i] ~ dbern(p[i])
		p[i] <- 1 / (1 + exp(-eta[i]))
		eta[i] <- int + x[i] * north
	}
	# prior
	int ~ dnorm(0, 1E-12)
	north ~ dnorm(0, 1E-12)
}
```

```{r, eval = F}
logistic.model <- "model{
  # likelihood
	for (i in 1:N){
		y[i] ~ dbern(p[i])
		p[i] <- 1 / (1 + exp(-eta[i]))
		eta[i] <- int + x[i] * north
	}
	# prior
	int ~ dnorm(0, 1E-12)
	north ~ dnorm(0, 1E-12)
}"

jags <- jags.model(textConnection(logistic.model),
                   data = list('y' = as.numeric(CO$Exceedance),
                               'N' = length(CO$Exceedance),
                               'x' = CO$north),
                   n.chains = 2,
                   n.adapt = 1000, quiet=T)
 
update(jags, 1000)
 
samp <- coda.samples(jags,
             c('int','north','p'),
             5000)
summary(samp)
```


## Spatial Logistic Regression
- Assume $Y(\boldsymbol{s_i})$ is the binary response for $\boldsymbol{s_i}$,
\begin{eqnarray*}
Y(\boldsymbol{s_i})|\beta, w(\boldsymbol{s_i}) &\sim& Bernoulli(\pi(\boldsymbol{s_i}))\\
logit(\pi(\boldsymbol{s_i})) &=& X(\boldsymbol{s_i}) \beta + w(\boldsymbol{s_i}), \\
\end{eqnarray*}
- where $\boldsymbol{W} \sim N(\boldsymbol{0},\sigma^2 H(\phi))$

## Spatial Logistic Regression in JAGS
```
logistic.spatial <- "model {
  for (i in 1:N) {
	  Y[i] ~ dbern(p[i])
	  logit(p[i]) <- mu[i] + w[i]
	  mu[i] <- beta[1]+beta[2]*x.north[i]
    muW[i] <- 0
  }
  # process
  w[1:N] ~ dmnorm(muW[],Omega[,])

  # priors
  beta[1] ~ dnorm(0.0,1E-8)
  beta[2] ~ dnorm(0.0,1E-8)
  spat.prec ~ dgamma(0.001, 0.001)
  sigmasq <- 1/spat.prec
  phi ~ dunif(1.5,2.5)	
	
  #build omega
  for (i in 1:N){
    for (j in 1:N){
      H[i,j] <- (1/ spat.prec) * exp(-phi * d[i,j])
    }
  }
  Omega[1:N,1:N] <- inverse(H[1:N,1:N])
}"
```

```{r, eval = F}
# Simulate Larger Spatial Data Set
N.sim <- 100
Lat.sim <- runif(N.sim,37,40)
Long.sim <- runif(N.sim,-109,-104)
phi.sim <- 2
sigmasq.sim <- .1
beta.sim <- c(0,2)
north.sim <-  as.numeric(Lat.sim > 38)


d <- dist(cbind(Lat.sim,Long.sim), upper = T, diag = T) %>% as.matrix
H.sim <- sigmasq.sim * exp(-phi.sim * d)
w.sim <- rmnorm(1,0,H.sim)
xb.sim <- beta.sim[1] + beta.sim[2] * north.sim
y.sim <- rbinom(N.sim,1,inv.logit(xb.sim + w.sim))

logistic.spatial <- "model {
  for (i in 1:N) {
	  Y[i] ~ dbern(p[i])
	  logit(p[i]) <- mu[i] + w[i]
	  mu[i] <- beta[1]+beta[2]*x.north[i]
    muW[i] <- 0
  }
  # process
  w[1:N] ~ dmnorm(muW[],Omega[,])

  # priors
  beta[1] ~ dnorm(0.0,1E-8)
  beta[2] ~ dnorm(0.0,1E-8)
  spat.prec ~ dgamma(0.001, 0.001)
  sigmasq <- 1/spat.prec
  phi ~ dunif(1.5,2.5)	
	
  #build omega
  for (i in 1:N){
    for (j in 1:N){
      H[i,j] <- (1/ spat.prec) * exp(-phi * d[i,j])
    }
  }
  Omega[1:N,1:N] <- inverse(H[1:N,1:N])
}"

model <- jags.model(textConnection(logistic.spatial), data = list('d' = d, 'N' = N.sim, x.north = north.sim, Y = y.sim))

update(model, 10000)

samp <- coda.samples(model, 
        variable.names=c("beta","phi", 'sigmasq'), 
        n.iter=5000)
summary(samp)
```



##  `spGLM()` 
```
m.1 <- spGLM(y~1, 
             family="binomial", 
             coords=coords, 
             weights=weights, 
             starting=list("beta"=beta.starting, "phi"=0.06,"sigma.sq"=1, "w"=0),
             tuning=list("beta"=beta.tuning, "phi"=0.5, "sigma.sq"=0.5, "w"=0.5),
             priors=list("beta.Normal"=list(0,10), "phi.Unif"=c(0.03, 0.3), "sigma.sq.IG"=c(2, 1)),
             amcmc=list("n.batch"=n.batch, "batch.length"=batch.length, "accept.rate"=0.43),
             cov.model="exponential", 
             verbose=TRUE, 
             n.report=10)
```

```{r, eval = F, echo = F}
##Generate binary data
coords <- as.matrix(expand.grid(seq(0,100,length.out=8), seq(0,100,length.out=8)))
n <- nrow(coords)

phi <- 3/50
sigma.sq <- 2

R <- sigma.sq*exp(-phi*as.matrix(dist(coords)))
w <- rmvn(1, rep(0,n), R)

x <- as.matrix(rep(1,n))
beta <- 0.1
p <- 1/(1+exp(-(x%*%beta+w)))

weights <- rep(1, n)

y <- rbinom(n, size=weights, prob=p)

##Collect samples
fit <- glm((y/weights)~x-1, weights=weights, family="binomial")
beta.starting <- coefficients(fit)
beta.tuning <- t(chol(vcov(fit)))

n.batch <- 200
batch.length <- 50
n.samples <- n.batch*batch.length

m.1 <- spGLM(y~1, family="binomial", coords=coords, weights=weights, 
             starting=list("beta"=beta.starting, "phi"=0.06,"sigma.sq"=1, "w"=0),
             tuning=list("beta"=beta.tuning, "phi"=0.5, "sigma.sq"=0.5, "w"=0.5),
             priors=list("beta.Normal"=list(0,10), "phi.Unif"=c(0.03, 0.3), "sigma.sq.IG"=c(2, 1)),
             amcmc=list("n.batch"=n.batch, "batch.length"=batch.length, "accept.rate"=0.43),
             cov.model="exponential", verbose=TRUE, n.report=10)

burn.in <- 0.9*n.samples
sub.samps <- burn.in:n.samples

print(summary(window(m.1$p.beta.theta.samples, start=burn.in)))

beta.hat <- m.1$p.beta.theta.samples[sub.samps,"(Intercept)"]
w.hat <- m.1$p.w.samples[,sub.samps]

p.hat <- 1/(1+exp(-(x%*%beta.hat+w.hat)))

y.hat <- apply(p.hat, 2, function(x){rbinom(n, size=weights, prob=p)})

y.hat.mu <- apply(y.hat, 1, mean)
y.hat.var <- apply(y.hat, 1, var)

##Take a look
par(mfrow=c(1,2))
surf <- mba.surf(cbind(coords,y.hat.mu),no.X=100, no.Y=100, extend=TRUE)$xyz.est
image(surf, main="Interpolated mean of posterior rate\n(observed rate)")
contour(surf, add=TRUE)
text(coords, label=paste("(",y,")",sep=""), cex=.5)

surf <- mba.surf(cbind(coords,y.hat.var),no.X=100, no.Y=100, extend=TRUE)$xyz.est
image(surf, main="Interpolated variance of posterior rate\n(observed #
of trials)")
contour(surf, add=TRUE)
text(coords, label=paste("(",weights,")",sep=""), cex=.5)
```

# Spatial Poisson Regression

## Motivation

```{r}
CO.colors <- ifelse(CO$Exceedance_Count == 0, 'green',
         ifelse(CO$Exceedance_Count <= 5, 'red',
                ifelse(CO$Exceedance_Count <= 10, 'darkpurple','black')))
  
icons <- awesomeIcons(
  icon = 'ios-close',
  iconColor = 'white',
  library = 'ion',
  markerColor = CO.colors
)

leaflet(CO) %>% addTiles() %>%
  addAwesomeMarkers(lng = ~Longitude, lat = ~Latitude, icon=icons, label=~as.character(CO$Exceedance_Count))
```


## Poisson Regression Overview
Write out the complete model specification for Poisson regression.

- Assume $Y_i$ is the count response for the $i^{th}$ observation,
\begin{eqnarray*}
Y_i &\sim& Poisson(\lambda_i)\\
\log(\lambda_i) &=& X_i \beta,
\end{eqnarray*}

- thus $\exp(X_i \beta) \geq 0$
- next write out a Poisson regression model with spatial random effects

## Spatial Poisson Regression
- Assume $Y_i$ is the count response for $\boldsymbol{s_i}$,
\begin{eqnarray*}
Y(\boldsymbol{s_i}) &\sim& Poisson(\lambda(\boldsymbol{s_i}))\\
\log(\lambda(\boldsymbol{s_i})) &=& X(\boldsymbol{s_i}) \beta + w(\boldsymbol{s_i}),
\end{eqnarray*}

- where $\boldsymbol{W} \sim N(\boldsymbol{0},\sigma^2 H(\phi))$

## Fitting Spatial Poisson Regression Models

1. `spGLM()` accepts binomial and Poisson data,
2. JAGS can also be used to fit spatial GLMS models, or
3. Try writing a sampler from scratch.

